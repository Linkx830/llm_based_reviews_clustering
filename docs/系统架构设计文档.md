# 系统架构设计文档

**新增或修改我时需要修改这个文件夹中的README.md文件**

## 文档说明

本文档详细描述基于电商评论的方面级情感分析、需求聚类与多Agent自动洞察系统的软件架构、设计模式和实现方法。

**文档版本**: v1.0  
**最后更新**: 2026-01-11

---

## 目录

1. [系统概述](#1-系统概述)
2. [架构设计](#2-架构设计)
3. [核心组件设计](#3-核心组件设计)
4. [数据流与处理流程](#4-数据流与处理流程)
5. [关键技术实现](#5-关键技术实现)
6. [设计模式与原则](#6-设计模式与原则)
7. [扩展性设计](#7-扩展性设计)
8. [性能与优化](#8-性能与优化)

---

## 1. 系统概述

### 1.1 系统目标

本系统是一个基于LangChain + DuckDB + LLM的数据挖掘系统，旨在从海量电商评论中：

- **抽取方面级情感**：识别评论中涉及的方面（aspect）及其对应的情感倾向
- **聚类痛点需求**：将相似的问题（issue）聚合成簇，形成可解释的痛点地图
- **生成可执行洞察**：为每个簇自动生成命名、摘要、证据和建议，支持产品改进决策

### 1.2 核心特性

- **多Agent架构**：采用LangChain Agent模式，实现模块化、可扩展的处理流程
- **可复现性**：通过版本字段（run_id, pipeline_version, data_slice_id）确保结果可复现
- **断点续跑**：支持从任意步骤恢复执行，提高开发效率
- **结构化输出**：使用Pydantic Schema确保LLM输出的稳定性和可验证性
- **两阶段聚类**：先按aspect分桶，再桶内聚类issue，避免跨维度混淆

### 1.3 技术栈

- **框架**: LangChain（Agent编排）
- **数据库**: DuckDB（列式存储，高性能分析）
- **LLM**: 支持OpenAI、Ollama等多种提供商
- **Embedding**: sentence-transformers、Qwen3-Embedding等
- **聚类**: HDBSCAN、AgglomerativeClustering
- **数据处理**: NumPy、Pandas、scikit-learn

---

## 2. 架构设计

### 2.1 分层架构

系统采用**分层架构**，从下至上分为：

```
┌─────────────────────────────────────────┐
│     应用层 (Application Layer)          │
│  - Orchestrator (编排器)                │
│  - Pipeline (流水线定义)                │
└─────────────────────────────────────────┘
           ↓
┌─────────────────────────────────────────┐
│     Agent层 (Agent Layer)              │
│  - DataSelectorAgent                   │
│  - AspectSentimentExtractorAgent       │
│  - IssueClusterAgent                   │
│  - ClusterInsightAgent                 │
│  - ... (共11个Agent)                   │
└─────────────────────────────────────────┘
           ↓
┌─────────────────────────────────────────┐
│     工具层 (Tool Layer)                 │
│  - EmbeddingTool                       │
│  - ClusteringTool                      │
│  - RerankerTool                        │
│  - StructuredOutputTool                │
│  - LoggingTool                         │
└─────────────────────────────────────────┘
           ↓
┌─────────────────────────────────────────┐
│     模型层 (Model Layer)                │
│  - LLMWrapper                          │
│  - EmbeddingWrapper                    │
└─────────────────────────────────────────┘
           ↓
┌─────────────────────────────────────────┐
│     存储层 (Storage Layer)              │
│  - DuckDBConnection                    │
│  - TableManager                        │
│  - VersionFields                       │
└─────────────────────────────────────────┘
           ↓
┌─────────────────────────────────────────┐
│     数据层 (Data Layer)                 │
│  - DuckDB Database                     │
│  - 中间表 (11张表)                      │
│  - 原始表 (reviews, meta)               │
└─────────────────────────────────────────┘
```

### 2.2 组件职责划分

#### 2.2.1 应用层

**Orchestrator（编排器）**
- 职责：控制执行顺序、统一写入DuckDB、断点续跑、日志记录
- 设计模式：Facade模式（为复杂的Agent系统提供统一接口）
- 关键特性：
  - 单写者模式（避免DuckDB并发写入问题）
  - 依赖管理（基于Pipeline的拓扑排序）
  - 版本一致性检查

**Pipeline（流水线定义）**
- 职责：定义Agent执行顺序和依赖关系
- 设计模式：Builder模式（通过PipelineStep构建流水线）
- 关键特性：
  - 拓扑排序（自动解析依赖）
  - 步骤配置（每个步骤可独立配置）

#### 2.2.2 Agent层

所有Agent继承自`BaseAgent`，遵循统一接口：

```python
class BaseAgent(ABC):
    def __init__(self, db_conn, run_id, pipeline_version, data_slice_id):
        # 统一初始化：数据库连接、版本字段
        
    @abstractmethod
    def process(self, **kwargs) -> Dict[str, Any]:
        # 子类必须实现的处理逻辑
```

**Agent分类**：

1. **数据准备类Agent**
   - `DataSelectorAgent`: 数据切片与实验样本固定
   - `MetaContextAgent`: 元数据上下文构建（截断/摘要）
   - `PreprocessAgent`: 评论级清洗与文本规范化
   - `SentenceBuilderAgent`: 句子构建 + 上下文窗口

2. **抽取类Agent**
   - `OpinionCandidateFilterAgent`: 观点候选句过滤（成本控制）
   - `AspectSentimentExtractorAgent`: LLM结构化抽取
   - `ExtractionJudgeAgent`: 抽取校验、归一、噪声处理

3. **聚类类Agent**
   - `IssueClusterAgent`: 两阶段聚类（aspect分桶 + 桶内issue聚类）

4. **洞察类Agent**
   - `ClusterInsightAgent`: 簇命名、摘要、建议、优先级

5. **输出类Agent**
   - `ReportAssemblerAgent`: 报告组装
   - `EvaluationAgent`: 自动指标 + 人工抽样包

#### 2.2.3 工具层

**EmbeddingTool**
- 职责：文本向量化
- 特性：
  - 支持instruction-aware embedding
  - 批量处理（batch_size可配置）
  - 并发控制（max_workers）
  - MRL维度裁剪支持

**ClusteringTool**
- 职责：聚类算法封装
- 支持算法：
  - HDBSCAN（推荐，适合长尾分布）
  - AgglomerativeClustering（适合小规模数据）
  - DBSCAN（备选）
- 功能：
  - 簇统计计算（簇内距离、簇间距离、分离度比率等）
  - 轮廓系数计算

**RerankerTool**
- 职责：二次验证，边界精修
- 特性：
  - 支持基于embedding的reranker
  - 支持LLM-based reranker（fallback）
  - 批量打分（提高效率）

**StructuredOutputTool**
- 职责：LLM结构化输出解析
- 特性：
  - 支持Pydantic Schema验证
  - JSON提取与修复
  - 重试机制

#### 2.2.4 模型层

**LLMWrapper**
- 职责：统一LLM接口
- 支持提供商：
  - OpenAI（GPT系列）
  - Ollama（本地部署）
- 特性：
  - 结构化输出（Function Calling / Pydantic）
  - 重试机制（指数退避）
  - Reasoning模式支持（可选）

**EmbeddingWrapper**
- 职责：Embedding模型封装
- 支持模型：
  - sentence-transformers系列
  - Qwen3-Embedding
  - 自定义模型（通过base_url）

#### 2.2.5 存储层

**DuckDBConnection**
- 职责：数据库连接管理
- 特性：
  - 单例模式（确保单连接）
  - 连接池管理
  - 事务支持

**TableManager**
- 职责：表结构管理
- 功能：
  - 表创建（11张中间表）
  - 表存在性检查
  - 版本字段管理

**VersionFields**
- 职责：版本字段生成与管理
- 字段：
  - `run_id`: 运行ID（时间戳+描述）
  - `pipeline_version`: 管道版本（v1.0）
  - `data_slice_id`: 数据切片ID
  - `created_at`: 创建时间

---

## 3. 核心组件设计

### 3.1 Orchestrator设计

**类图**：

```
Orchestrator
├── db_conn: DuckDBConnection
├── table_manager: TableManager
├── logger: LoggingTool
├── run_id: str
├── pipeline_version: str
├── data_slice_id: str
│
├── run_pipeline()
│   ├── 创建输出目录
│   ├── 保存运行配置
│   ├── 设置日志
│   ├── 执行Pipeline步骤（拓扑排序）
│   │   ├── 创建Agent实例
│   │   ├── 调用agent.process()
│   │   ├── 记录成功/失败
│   │   └── 更新data_slice_id
│   └── 返回结果
│
└── _save_run_config()
```

**关键设计决策**：

1. **单写者模式**：所有Agent只读不写，由Orchestrator统一写入，避免DuckDB并发问题
2. **断点续跑**：通过`resume_from`参数跳过已完成的步骤
3. **配置保存**：每次运行保存完整配置到`outputs/runs/<run_id>/config/`

### 3.2 IssueClusterAgent设计（核心聚类组件）

**类图**：

```
IssueClusterAgent extends BaseAgent
├── embedding_tool: EmbeddingTool
├── reranker_tool: RerankerTool (可选)
├── use_instruction: bool
├── aspect_similarity_threshold: float
│
├── process()
│   ├── 阶段A: 构建结构化输入文本
│   ├── 阶段B: 生成两路向量 (E_issue, E_aspect)
│   ├── 阶段C1: Aspect分桶（合并同义aspect）
│   ├── 阶段C2: 桶内对Issue聚类
│   ├── 阶段D: Reranker边界精修（可选）
│   ├── 阶段E: 簇后处理（噪点吸附、小簇合并）
│   └── 插入数据库
│
├── _build_cluster_text()
├── _merge_similar_aspects()
├── _select_medoid()
├── _reranker_refinement()
└── _post_process_clusters()
```

**两阶段聚类流程**：

```
输入: aspect_sentiment_valid (VALID记录)
  ↓
阶段A: 构建结构化文本
  "Aspect: {aspect_norm}\nIssue: {issue_norm}"
  ↓
阶段B: 生成两路向量
  E_issue = embed(cluster_text, instruction="Represent issue type...")
  E_aspect = embed(aspect_norm, instruction="Represent aspect...")
  ↓
阶段C1: Aspect分桶
  1. 计算aspect相似度矩阵
  2. AgglomerativeClustering合并同义aspect
  3. 选择频次最高的aspect作为标准aspect
  ↓
阶段C2: 桶内Issue聚类
  对每个aspect桶:
    1. 提取桶内issue向量
    2. 根据数据量选择算法:
       - < 1000: AgglomerativeClustering
       - >= 1000: HDBSCAN
    3. 执行聚类
  ↓
阶段D: Reranker精修（可选）
  1. 对每个样本找top-k近邻
  2. Reranker打分
  3. 构建图（边=reranker高分对）
  4. 连通分量聚类
  ↓
阶段E: 后处理
  1. 噪点吸附（相似度阈值）
  2. 小簇合并/标记为噪声
  3. 选择medoid
  4. 计算簇统计
  ↓
输出: issue_clusters, cluster_stats
```

**关键设计决策**：

1. **结构化输入**：使用`Aspect: {aspect}\nIssue: {issue}`格式，提高embedding质量
2. **两路向量**：分别对aspect和issue建模，支持分桶策略
3. **自适应算法选择**：根据数据量自动选择HDBSCAN或Agglomerative
4. **Reranker精修**：可选但强力的边界精修机制

### 3.3 ClusterInsightAgent设计

**类图**：

```
ClusterInsightAgent extends BaseAgent
├── llm: LLMWrapper
├── output_tool: StructuredOutputTool
├── prompt_template: str
│
├── process()
│   ├── 读取cluster_stats
│   ├── 对每个簇:
│   │   ├── 获取代表样本
│   │   ├── 构建prompt
│   │   ├── 调用LLM生成洞察
│   │   ├── 验证证据可回溯性
│   │   └── 插入cluster_reports
│   └── 返回统计
│
├── _build_prompt()
├── _generate_insight_with_llm()
├── _fix_llm_output_format()
├── _validate_evidence_items()
└── _generate_insight_simple()  # Fallback
```

**输出Schema**：

```python
class ClusterInsightOutput(BaseModel):
    cluster_name: str  # 簇名称（≤20字）
    summary: str  # 2-3句现象总结
    priority: str  # high/medium/low
    priority_rationale: Optional[str]
    evidence_items: List[EvidenceItem]  # 证据条目（可回溯）
    action_items: List[ActionItem]  # 可执行建议
    risks_and_assumptions: Optional[List[str]]
    confidence: Optional[float]  # 0~1
```

**关键设计决策**：

1. **结构化输出**：使用Pydantic Schema确保输出格式稳定
2. **证据可回溯性**：验证evidence_items中的sentence_id必须来自输入样本
3. **格式修复**：自动修复LLM输出格式问题（字符串数组→对象数组）
4. **Fallback机制**：LLM失败时使用简化版本

### 3.4 数据表设计

**核心中间表**（11张）：

1. `selected_reviews`: 实验样本固定表
2. `meta_context`: 元数据上下文（截断/摘要后）
3. `normalized_reviews`: 规范化后的评论
4. `review_sentences`: 句子表（含上下文窗口）
5. `opinion_candidates`: 观点候选句标记
6. `aspect_sentiment_raw`: LLM原始抽取结果
7. `aspect_sentiment_valid`: 校验后的方面情感事实表
8. `extraction_issues`: 抽取问题记录
9. `issue_clusters`: 聚类归属表
10. `cluster_stats`: 簇统计表
11. `cluster_reports`: 簇洞察表

**版本字段设计**：

所有表都包含以下版本字段：
- `run_id`: 运行ID（唯一标识一次运行）
- `pipeline_version`: 管道版本（v1.0）
- `data_slice_id`: 数据切片ID
- `created_at`: 创建时间

**设计原则**：
- 每张表都是不可变的（append-only）
- 通过`run_id`过滤实现版本隔离
- 支持多版本对比分析

---

## 4. 数据流与处理流程

### 4.1 完整数据流

```
原始数据 (reviews, meta)
  ↓
[DataSelectorAgent] 数据切片
  → selected_reviews
  ↓
[MetaContextAgent] 元数据上下文构建
  → meta_context
  ↓
[PreprocessAgent] 文本规范化
  → normalized_reviews
  ↓
[SentenceBuilderAgent] 句子拆分 + 上下文
  → review_sentences
  ↓
[OpinionCandidateFilterAgent] 观点过滤
  → opinion_candidates
  ↓
[AspectSentimentExtractorAgent] LLM抽取
  → aspect_sentiment_raw
  ↓
[ExtractionJudgeAgent] 校验 + 归一
  → aspect_sentiment_valid
  ↓
[IssueClusterAgent] 两阶段聚类
  → issue_clusters, cluster_stats
  ↓
[ClusterInsightAgent] 生成洞察
  → cluster_reports
  ↓
[EvaluationAgent] 评估指标
  → evaluation_metrics
  ↓
[ReportAssemblerAgent] 报告组装
  → final_report.md
```

### 4.2 关键流程详解

#### 4.2.1 抽取流程

```
review_sentences (含context_text)
  ↓
[OpinionCandidateFilterAgent] 过滤
  - 规则：句长、评价词、否定词、比较词
  - 输出：is_candidate标记
  ↓
[AspectSentimentExtractorAgent] LLM抽取
  - 输入：context_text + target_sentence + meta_context
  - 输出：结构化JSON（aspects列表）
  - 重试：3次，指数退避
  ↓
[ExtractionJudgeAgent] 校验
  - 结构校验
  - Aspect同义归一（taxonomy）
  - 噪声识别（泛化词过滤）
  - 证据可定位性检查
  ↓
aspect_sentiment_valid (VALID记录)
```

#### 4.2.2 聚类流程

详见[3.2 IssueClusterAgent设计](#32-issueclusteragent设计核心聚类组件)

#### 4.2.3 洞察生成流程

```
cluster_stats (含representative_sentence_ids)
  ↓
[ClusterInsightAgent] 对每个簇:
  1. 读取代表样本（最多20个）
  2. 构建prompt（aspect, cluster_id, size, neg_ratio, samples）
  3. 调用LLM生成洞察
  4. 验证evidence_items可回溯性
     - sentence_id必须在输入样本中
     - 如果为空，通过quote文本匹配
  5. 插入cluster_reports
  ↓
cluster_reports (含cluster_name, summary, action_items等)
```

---

## 5. 关键技术实现

### 5.1 结构化输出稳定性

**问题**：LLM输出格式不稳定，JSON解析失败率高

**解决方案**：

1. **优先使用Function Calling / Tool Call**
   - OpenAI: `function_calling`
   - Ollama: 通过prompt engineering实现结构化输出

2. **StructuredOutputTool自动修复**
   ```python
   def _fix_llm_output_format(self, response_text: str):
       # 1. 提取JSON（处理markdown代码块）
       json_str = self._extract_json(response_text)
       # 2. 修复字符串数组→对象数组
       if evidence_items是字符串数组:
           转换为对象数组（sentence_id先设为空）
       # 3. 确保必需字段存在
       # 4. 移除无效项
   ```

3. **重试机制**
   - 默认3次重试
   - 指数退避（1s, 2s, 4s）
   - 记录失败原因（error_type）

### 5.2 上下文丢失问题

**问题**：句子级处理导致上下文丢失（指代不明、转折丢失）

**解决方案**：

1. **SentenceBuilderAgent构建上下文窗口**
   ```python
   context_text = prev_sentence + " " + target_sentence + " " + next_sentence
   ```

2. **LLM抽取时明确约束**
   - Prompt中明确：`context_text`仅用于理解上下文
   - 只针对`target_sentence`做抽取
   - 证据必须可在`target_sentence`中定位

### 5.3 两阶段聚类实现

**阶段C1: Aspect分桶**

```python
def _merge_similar_aspects(aspect_norms, aspect_vectors):
    # 1. 计算cosine相似度矩阵
    similarity_matrix = np.dot(aspect_vectors, aspect_vectors.T)
    distance_matrix = 1 - similarity_matrix
    
    # 2. AgglomerativeClustering合并
    clustering = AgglomerativeClustering(
        distance_threshold=1 - aspect_similarity_threshold,
        linkage="average",
        metric="precomputed"
    )
    labels = clustering.fit_predict(distance_matrix)
    
    # 3. 选择标准aspect（频次最高）
    for cluster in unique_labels:
        canonical = Counter(cluster_aspects).most_common(1)[0][0]
```

**阶段C2: 桶内Issue聚类**

```python
for canonical_aspect, records in aspect_buckets.items():
    bucket_vectors = extract_issue_vectors(records)
    
    # 自适应算法选择
    if bucket_size < 1000:
        method = "agglomerative"
        config = {"distance_threshold": 0.5}
    else:
        method = "hdbscan"
        config = {"min_cluster_size": max(10, bucket_size * 0.005)}
    
    labels = ClusteringTool(method=method, **config).fit(bucket_vectors)
```

### 5.4 Reranker边界精修

**实现步骤**：

1. **候选对生成**
   ```python
   # 对每个样本找top-k近邻（基于embedding）
   nbrs = NearestNeighbors(n_neighbors=top_k+1, metric='cosine')
   neighbor_indices = nbrs.kneighbors([vector])[0][1:top_k+1]
   ```

2. **Reranker打分**
   ```python
   scores = reranker_tool.rerank_pairs(
       query_texts=[cluster_text[i] for i in range(n)],
       document_texts=[cluster_text[j] for j in neighbor_indices]
   )
   ```

3. **图构建与聚类**
   ```python
   # 构建图：边=reranker高分对（score >= threshold）
   graph = defaultdict(dict)
   for i, j, score in high_score_pairs:
       graph[i][j] = score
   
   # 连通分量聚类
   labels = find_connected_components(graph)
   ```

### 5.5 噪点簇和小簇后处理

**噪点吸附**：

```python
# 1. 找到每个簇的medoid
cluster_medoids = {cid: select_medoid(vectors, labels, cid) for cid in clusters}

# 2. 对噪声点，找最近的簇medoid
noise_vectors = vectors[noise_indices]
medoid_vectors = [vectors[medoids[cid]] for cid in clusters]
similarities = np.dot(noise_vectors, medoid_vectors.T)

# 3. 如果相似度 >= threshold，吸附到该簇
for noise_idx, max_sim in enumerate(similarities):
    if max_sim >= noise_adsorption_threshold:
        labels[noise_idx] = target_cluster
```

**小簇处理**：

```python
# 1. 识别小簇（size < min_cluster_size）
small_clusters = [cid for cid in clusters if size(cid) < min_cluster_size]

# 2. 尝试合并到相似的大簇
for small_cid in small_clusters:
    small_medoid = select_medoid(vectors, labels, small_cid)
    similarities = compute_similarity_to_large_clusters(small_medoid)
    
    if max_similarity >= small_cluster_merge_threshold:
        merge(small_cid, target_cluster)
    else:
        mark_as_noise(small_cid)
```

---

## 6. 设计模式与原则

### 6.1 使用的设计模式

1. **模板方法模式（Template Method）**
   - `BaseAgent`定义统一接口，子类实现`process()`方法

2. **策略模式（Strategy）**
   - `ClusteringTool`支持多种聚类算法（HDBSCAN、Agglomerative等）
   - 根据数据量自动选择策略

3. **单例模式（Singleton）**
   - `DuckDBConnection`使用单例确保单连接

4. **外观模式（Facade）**
   - `Orchestrator`为复杂的Agent系统提供统一接口

5. **建造者模式（Builder）**
   - `Pipeline`通过`PipelineStep`构建流水线

6. **适配器模式（Adapter）**
   - `LLMWrapper`统一不同LLM提供商的接口

### 6.2 设计原则

1. **单一职责原则（SRP）**
   - 每个Agent只负责一个明确的任务
   - 例如：`DataSelectorAgent`只负责数据切片

2. **开闭原则（OCP）**
   - 通过继承`BaseAgent`扩展新功能
   - 通过配置而非修改代码改变行为

3. **依赖倒置原则（DIP）**
   - Agent依赖抽象的`DuckDBConnection`，而非具体实现
   - 工具类通过接口注入，而非硬编码

4. **接口隔离原则（ISP）**
   - `BaseAgent`只定义必要的方法（`process()`）
   - 子类按需实现，不强制实现不需要的方法

5. **DRY原则（Don't Repeat Yourself）**
   - 版本字段通过`VersionFields`统一生成
   - 表结构通过`TableManager`统一管理

---

## 7. 扩展性设计

### 7.1 新增Agent

**步骤**：

1. 继承`BaseAgent`
2. 实现`process()`方法
3. 在`main.py`的`create_pipeline()`中添加`PipelineStep`
4. 定义依赖关系

**示例**：

```python
class CustomAgent(BaseAgent):
    def process(self, **kwargs):
        # 1. 读取上游数据
        query = f"SELECT * FROM {TableManager.UPSTREAM_TABLE} WHERE run_id = ?"
        data = self.db.execute_read(query, [self.run_id])
        
        # 2. 处理逻辑
        results = self._process_data(data)
        
        # 3. 返回结果（Orchestrator会统一写入）
        return {"status": "success", "processed_count": len(results)}
```

### 7.2 新增聚类算法

**步骤**：

1. 在`ClusteringTool.__init__()`中添加新方法
2. 在`fit()`方法中添加分支
3. 确保输出格式一致（返回labels数组）

**示例**：

```python
elif self.method == "custom_algorithm":
    from custom_clustering import CustomClustering
    self.model = CustomClustering(**self.kwargs)
    self.labels_ = self.model.fit_predict(vectors)
```

### 7.3 新增LLM提供商

**步骤**：

1. 在`LLMWrapper.__init__()`中添加新provider
2. 实现统一的接口方法：
   - `invoke()`: 普通调用
   - `invoke_structured()`: 结构化输出

**示例**：

```python
elif provider == "custom_llm":
    from custom_llm_client import CustomLLMClient
    self.client = CustomLLMClient(api_key=api_key, base_url=base_url)
```

### 7.4 新增数据表

**步骤**：

1. 在`TableManager`中添加表名常量
2. 实现`create_xxx_table()`方法
3. 在`create_all_tables()`中调用

**示例**：

```python
CUSTOM_TABLE = "custom_table"

def create_custom_table(self):
    schema = """
        run_id VARCHAR NOT NULL,
        pipeline_version VARCHAR NOT NULL,
        ...
    """
    self.db.create_table_if_not_exists(self.CUSTOM_TABLE, schema)
```

---

## 8. 性能与优化

### 8.1 性能优化策略

1. **批量处理**
   - Embedding: `batch_size=32`（可配置）
   - LLM抽取: 单条处理（保证质量）

2. **并发控制**
   - Embedding: `max_workers=4`（可配置）
   - Reranker: `max_workers=4`（可配置）

3. **缓存机制**
   - 相同`sentence_id`不重复抽取（通过数据库检查）
   - Embedding结果可缓存（相同文本）

4. **向量化优化**
   - 使用NumPy向量化操作
   - 避免Python循环（使用NumPy广播）

### 8.2 成本控制

1. **前置过滤**
   - `OpinionCandidateFilterAgent`过滤掉非观点句
   - 减少LLM调用量（通常可减少50-70%）

2. **MRL维度裁剪**
   - 支持降低embedding维度（768→256）
   - 减少存储和计算成本

3. **Reranker选择性使用**
   - 只对边界样本使用reranker
   - 通过`use_reranker`配置开关

### 8.3 可扩展性

1. **水平扩展**
   - Agent可独立部署（通过消息队列）
   - DuckDB支持多读单写

2. **垂直扩展**
   - 支持更大规模数据（通过数据切片）
   - 支持更复杂的模型（通过配置）

3. **功能扩展**
   - 新增Agent不影响现有流程
   - 通过配置而非代码修改

---

## 附录

### A. 关键配置项

**运行配置** (`configs/runs/*.yaml`):
- `data_slice`: 数据切片配置
- `models`: 模型配置（LLM、Embedding）
- `prompts`: Prompt版本
- `clustering`: 聚类参数
- `judge`: 校验参数

**环境变量** (`.env`):
- `LLM_PROVIDER`: LLM提供商
- `LLM_MODEL`: 模型名称
- `LLM_BASE_URL`: API地址
- `API_KEY`: API密钥
- `DUCKDB_PATH`: 数据库路径

### B. 关键文件位置

- **主程序**: `main.py`
- **Orchestrator**: `src/app/orchestrator.py`
- **Pipeline定义**: `src/pipelines/pipeline.py`
- **Agent实现**: `src/agents/*.py`
- **工具类**: `src/tools/*.py`
- **表管理**: `src/storage/table_manager.py`
- **配置加载**: `src/utils/config_loader.py`

### C. 相关文档

- **软件设计文档**: `docs/软件设计文档.md`（业务设计）
- **聚类规范**: `docs/聚类规范.md`（聚类算法详细说明）
- **Schema规范**: `docs/附录 A：LLM 结构化输出 Schema 规范.md`
- **表规范**: `docs/附录 B：DuckDB 中间表与版本字段规范.md`

---

**文档结束**

